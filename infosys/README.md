## Infosys (Role: BI Hadoop | Location: Gurgaon | Date: 4th Jan 2020)
This was the very weird experience with Infosys, why because although I had went to their Gurgaon office from Noida (2 hours journey), but I gave a telephonic interview there. And intersting thing that, I was very much positive after that round, but they asked me to leave without sharing any feedback (might be they wanted to take cheap labor :P, just kidding, but not sure why they did this.) 

Ok, lets come to the questions that have been asked.
* Project experience and questions based on that.
* How to handle data failure and a job failure in ETL pipelines implemented using spark.
* How to reprocess the data. (For example transaction data does not make sense if not correlated with customer info, so the scenario was that some of the customer info come very very late than transaction data, so what you do to reprocess that customer data so that you can have complete info of the transactions and who made them by end of the day.)
* Types of file formats you worked and why you chose one particular. (Wanted to know difference b/w avro and parquet).
* We have a very very large file, how to process it in spark.
* Tell me the errors you have faced in running spark job and how did you solve them. (Kind of wanted to know spark performance tunning concepts).
* Explain concept of partitioning and bucketing with example and which column you would choose for partitioning and which for bucketing.
